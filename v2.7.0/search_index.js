var documenterSearchIndex = {"docs":
[{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using JsonGrinder","category":"page"},{"location":"tools/hierarchical/#HierarchicalUtils.jl","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"JsonGrinder.jl uses HierarchicalUtils.jl which brings a lot of additional features.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using HierarchicalUtils","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Let's say we have a complex Schema, which we want to further inspect:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using JSON\njss = JSON.parse(\"\"\"[\n    { \"a\": { \"b\": \"foo\", \"c\": [5, 6] }, \"d\": \"bar\" },\n    { \"d\": \"baz\" },\n    { \"a\": { \"c\": [] }, \"b\": \"foo\" }\n]\"\"\");\n\nsch = schema(jss)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"In small enough schema, all types of nodes are visible, but it gets more complicated if the schema does not fit your screen. Let's see how we can use HierarchicalUtils.jl to programmatically examine sch.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"First, the whole tree (regardless of the display area size) can be printed with","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"printtree(sch)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"printtree(sch, trav=true)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This way any element in the schema is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes in the tree. All tree nodes are accessible by indexing with the traversal code:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"sch[\"O\"]","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"The following two approaches give the same result:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"sch[\"O\"] ≡ sch[:a][:c].items","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"We can iterate over specific nodes in the schema. Let's for example collect all its leaves:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"LeafIterator(sch) |> collect","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"or all DictEntry nodes:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"TypeIterator(DictEntry, sch) |> collect","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"We can for example get all traversal codes for nodes matching a given predicate:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"codes = pred_traversal(sch, n -> n.updated ≥ 2)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"We can even get Accessors.jl optics:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"optic = code2lens(sch, \"M\") |> only","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"which can be used to access the nodes too (as well as many other operations):","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using Accessors","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"getall(sch, optic) |> only","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"ukn: Further reading\nFor the complete showcase of possibilities, refer to the HierarchicalUtils.jl manual.","category":"page"},{"location":"api/schema/#schema_api","page":"Schema","title":"Schema API","text":"","category":"section"},{"location":"api/schema/#Index","page":"Schema","title":"Index","text":"","category":"section"},{"location":"api/schema/","page":"Schema","title":"Schema","text":"Pages = [\"schema.md\"]","category":"page"},{"location":"api/schema/#API","page":"Schema","title":"API","text":"","category":"section"},{"location":"api/schema/","page":"Schema","title":"Schema","text":"schema\nupdate!\nnewentry\n\nBase.merge\nBase.merge!\n\nSchema\nLeafEntry\nDictEntry\nArrayEntry\n\nJsonGrinder.max_values\nJsonGrinder.max_values!\n\nJsonGrinder.max_string_codeunits\nJsonGrinder.max_string_codeunits!","category":"page"},{"location":"api/schema/#JsonGrinder.schema","page":"Schema","title":"JsonGrinder.schema","text":"schema([f=identity,] jsons)\n\nCreate schema from an iterable of jsons optionally mapped by function f.\n\nSee also: merge, merge!.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.update!","page":"Schema","title":"JsonGrinder.update!","text":"update!(e, v)\n\nUpdate the Schema e with value v and return the resulting entry.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.newentry","page":"Schema","title":"JsonGrinder.newentry","text":"newentry(v)\n\nCreate and return a new Schema according to the type of v.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#Base.merge","page":"Schema","title":"Base.merge","text":"merge(schemas...)\n\nMerge multiple schemas into one.\n\nUseful when for example distributing calculation of schema across multiple workers to aggregate all results.\n\nSee also: merge!, schema.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#Base.merge!","page":"Schema","title":"Base.merge!","text":"merge!(schema, others...)\n\nMerge multiple schemas others into schema inplace.\n\nSee also: merge, schema.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.Schema","page":"Schema","title":"JsonGrinder.Schema","text":"Schema\n\nSupertype for all schema node types.\n\n\n\n\n\n","category":"type"},{"location":"api/schema/#JsonGrinder.LeafEntry","page":"Schema","title":"JsonGrinder.LeafEntry","text":"LeafEntry{T} <: Schema\n\nKeeps statistics about scalar values of type T in leaves.\n\n\n\n\n\n","category":"type"},{"location":"api/schema/#JsonGrinder.DictEntry","page":"Schema","title":"JsonGrinder.DictEntry","text":"DictEntry <: Schema\n\nKeeps statistics about JSON \"objects\" containing key-value pairs.\n\n\n\n\n\n","category":"type"},{"location":"api/schema/#JsonGrinder.ArrayEntry","page":"Schema","title":"JsonGrinder.ArrayEntry","text":"ArrayEntry <: Schema\n\nKeeps statistics about an \"array\" entry in JSONs.\n\n\n\n\n\n","category":"type"},{"location":"api/schema/#JsonGrinder.max_values","page":"Schema","title":"JsonGrinder.max_values","text":"JsonGrinder.max_values!(n::Int)\n\nGet the current value of the max_values parameter.\n\nSee also: JsonGrinder.max_values!.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.max_values!","page":"Schema","title":"JsonGrinder.max_values!","text":"JsonGrinder.max_values!(n::Int; persist=false)\n\nSet the value of the max_values parameter.\n\nSet persist=true to persist this setting between sessions.\n\nSee also: JsonGrinder.max_values.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.max_string_codeunits","page":"Schema","title":"JsonGrinder.max_string_codeunits","text":"JsonGrinder.max_string_codeunits!(n::Int)\n\nGet the current value of the max_string_codeunits parameter.\n\nSee also: JsonGrinder.max_string_codeunits!.\n\n\n\n\n\n","category":"function"},{"location":"api/schema/#JsonGrinder.max_string_codeunits!","page":"Schema","title":"JsonGrinder.max_string_codeunits!","text":"JsonGrinder.max_string_codeunits!(n::Int; persist=false)\n\nSet the value of the max_string_codeunits parameter.\n\nSet persist=true to persist this setting between sessions.\n\nSee also: JsonGrinder.max_string_codeunits.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#utilities_api","page":"Utilities API","title":"Utilities API","text":"","category":"section"},{"location":"api/utilities/#Index","page":"Utilities API","title":"Index","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities API","title":"Utilities API","text":"Pages = [\"utilities.md\"]","category":"page"},{"location":"api/utilities/#API","page":"Utilities API","title":"API","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities API","title":"Utilities API","text":"Mill.reflectinmodel\n\nremove_nulls\nmap_keys","category":"page"},{"location":"api/utilities/#Mill.reflectinmodel","page":"Utilities API","title":"Mill.reflectinmodel","text":"reflectinmodel(sch::Schema, ex::Extractor, args...; kwargs...)\n\nUsing schema sch and extractor ex, first create a representative sample and then call Mill.reflectinmodel.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#JsonGrinder.remove_nulls","page":"Utilities API","title":"JsonGrinder.remove_nulls","text":"remove_nulls(js)\n\nReturn a new document in which all null values (represented as nothing in julia) are removed.\n\nExamples\n\njulia> remove_nulls(Dict(\"a\" => 1, \"b\" => nothing))\nDict{String, Union{Nothing, Int64}} with 1 entry:\n  \"a\" => 1\n\njulia> [nothing, Dict(\"a\" => 1), nothing, Dict(\"a\" => nothing)] |> remove_nulls\n2-element Vector{Dict{String}}:\n Dict(\"a\" => 1)\n Dict{String, Nothing}()\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#JsonGrinder.map_keys","page":"Utilities API","title":"JsonGrinder.map_keys","text":"map_keys(f, d)\n\nReturn a new document in which all keys are (recursively) transformed by callable f.\n\nExamples\n\njulia> d = Dict(\"a\" => 1, \"b\" => Dict(\"c\" => \"foo\"))\nDict{String, Any} with 2 entries:\n  \"b\" => Dict(\"c\"=>\"foo\")\n  \"a\" => 1\n\njulia> map_keys(Symbol, d)\nDict{Symbol, Any} with 2 entries:\n  :a => 1\n  :b => Dict(:c=>\"foo\")\n\njulia> map_keys(string, d)\nDict{String, Any} with 2 entries:\n  \"b\" => Dict(\"c\"=>\"foo\")\n  \"a\" => 1\n\n\n\n\n\n","category":"function"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"using Pkg\nold_path = Pkg.project().path\nPkg.activate(pwd())\nPkg.instantiate()","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"EditURL = \"mutagenesis_literate.jl\"","category":"page"},{"location":"examples/mutagenesis/mutagenesis/#Mutagenesis","page":"Mutagenesis","title":"Mutagenesis","text":"","category":"section"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"This example demonstrates how to predict the mutagenicity on Salmonella typhimurium.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"ukn: Jupyter notebook\nThis example is also available as a Jupyter notebook and the environment and the data are accessible here.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We load all dependencies and fix the seed:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"using JsonGrinder, Mill, Flux, JSON, MLUtils, Statistics\n\nusing Random; Random.seed!(42);\nnothing #hide","category":"page"},{"location":"examples/mutagenesis/mutagenesis/#Loading-the-data","page":"Mutagenesis","title":"Loading the data","text":"","category":"section"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"we load the dataset (available ), and split it into training and testing set.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"dataset = JSON.parsefile(\"mutagenesis.json\");\njss_train, jss_test = dataset[1:100], dataset[101:end];\nnothing #hide","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"jss_train and jss_test are just lists of parsed JSONs:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"jss_train[1]","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We also extract binary labels, which are stored in the \"mutagenic\" key:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"y_train = getindex.(jss_train, \"mutagenic\");\ny_test = getindex.(jss_test, \"mutagenic\");\ny_train","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We first create the schema of the training data, which is the first important step in using the JsonGrinder.jl. This infers both the hierarchical structure of the documents and basic statistics of individual values.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"sch = schema(jss_train)","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Of course, we have to remove the \"mutagenic\" key from the schema, as we don't want to include it in the data:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"delete!(sch, :mutagenic);\nsch","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Now we create an extractor capable of converting JSONs to Mill.jl structures. We use function suggestextractor with the default settings:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"e = suggestextractor(sch)","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We also need to convert JSONs to Mill.jl data samples. Extractor e is callable, we can use it to extract one document as follows:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"x_single = e(jss_train[1])","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"To extract a batch of 10 documents, we can extract individual documents and then Mill.catobs them:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"x_batch = reduce(catobs, e.(jss_train[1:10]))","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Or we can use a much more efficient extract function, which operates on a list of documents: Because the dataset is small, we can extract all data at once and keep it in memory:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"x_train = extract(e, jss_train);\nx_test = extract(e, jss_test);\nx_train","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Then we create an encoding model capable of embedding each JSON document into a fixed-size vector.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"encoder = reflectinmodel(sch, e)","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"ukn: Further reading\nFor further details about reflectinmodel, see the Mill.jl documentation.","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Finally, we chain the encoder with one more dense layer computing the logit of mutagenic probability:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"model = vec ∘ Dense(10, 1) ∘ encoder","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We can train the model in the standard Flux.jl way. We define the loss function, optimizer, and minibatch iterator:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"pred(m, x) = σ.(m(x))\nloss(m, x, y) = Flux.Losses.logitbinarycrossentropy(m(x), y);\nopt_state = Flux.setup(Flux.Optimise.Descent(), model);\nminibatch_iterator = Flux.DataLoader((x_train, y_train), batchsize=32, shuffle=true);\nnothing #hide","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We train for 10 epochs, and after each epoch we report the training accuracy:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"accuracy(p, y) = mean((p .> 0.5) .== y)\nfor i in 1:10\n    Flux.train!(loss, model, minibatch_iterator, opt_state)\n    @info \"Epoch $i\" accuracy=accuracy(pred(model, x_train), y_train)\nend","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"We can compute the accuracy on the testing set now:","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"accuracy(pred(model, x_test), y_test)","category":"page"},{"location":"examples/mutagenesis/mutagenesis/","page":"Mutagenesis","title":"Mutagenesis","text":"Pkg.activate(old_path)","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"using JsonGrinder","category":"page"},{"location":"tools/hyperopt/#Integrating-with-Hyperopt.jl","page":"Hyperopt.jl","title":"Integrating with Hyperopt.jl","text":"","category":"section"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"Below, we show a simple example of how to use Hyperopt.jl to perform hyperparameter optimization for us.","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"ukn: Prerequisites\nWe reuse a lot of code from the Recipe Ingredients example and recommend the reader to get familiar with it.The data are accessible here.","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"We import all libraries, split the dataset into training, testing, and validation sets, prepare one-hot-encoded labels, infer a Schema, define an Extractor, and extract all documents:","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"using Mill, Flux, OneHotArrays, JSON, Statistics\n\nusing Random; Random.seed!(42);\n\nnothing # hide","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"dataset = JSON.parse.(readlines(\"../examples/recipes/recipes.jsonl\"))\nshuffle!(dataset)\njss_train, jss_val, jss_test = dataset[1:1500], dataset[1501:2000], dataset[2001:end]\n\ny_train = getindex.(jss_train, \"cuisine\")\ny_val = getindex.(jss_val, \"cuisine\")\ny_test = getindex.(jss_test, \"cuisine\")\nclasses = unique(y_train)\ny_train_oh = onehotbatch(y_train, classes)\n\nsch = schema(jss_train)\ndelete!(sch.children, :cuisine)\ndelete!(sch.children, :id)\n\ne = suggestextractor(sch)\n\nx_train = extract(e, jss_train)\nx_val = extract(e, jss_val)\nx_test = extract(e, jss_test)\n\npred(m, x) = softmax(m(x))\naccuracy(p, y) = mean(onecold(p, classes) .== y)\nloss(m, x, y) = Flux.Losses.logitbinarycrossentropy(m(x), y)\n\nnothing # hide","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"Now we define a function train_model, which given a set of hyperparameters trains a new model. We will use the following hyperparameters:","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"epochs: number of epochs in training\nbatchsize: number of samples in a single minibatch\nd: \"inner\" dimensionality of Dense layers in the models\nlayers: number of layers to use in each node in the model\nactivation: activation function","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"function train_model(epochs, batchsize, d, layers, activation)\n    layer_builder = in_d -> Chain(\n        Dense(in_d, d, activation), [Dense(d, d, activation) for _ in 1:layers-1]...\n    )\n    encoder = reflectinmodel(sch, e, layer_builder)\n    model = Dense(d, length(classes)) ∘ encoder\n\n    opt_state = Flux.setup(Flux.Optimise.Adam(), model);\n    minibatch_iterator = Flux.DataLoader((x_train, y_train_oh); batchsize, shuffle=true);\n\n    for i in 1:epochs\n        Flux.train!(loss, model, minibatch_iterator, opt_state)\n    end\n\n    model\nend\n\nnothing # hide","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"train_model(2, 20, 10, 2, identity)","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"Now we can run the hyperparameter search. In this simple example, we will use RandomSampler, and in each iteration we will train a new model with train_model. The optimization criterion is the accuracy on the validation set.","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"using Hyperopt","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"ho = @hyperopt for i = 50,\n            sampler = RandomSampler(),\n            epochs = [3, 5, 10],\n            batchsize = [16, 32, 64],\n            d = [16, 32, 64],\n            layers = [1, 2, 3],\n            activation = [identity, relu, tanh]\n    model = train_model(epochs, batchsize, d, layers, activation)\n    accuracy(pred(model, x_val), y_val)\nend","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"We have arrived at the following solution:","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"printmax(ho)","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"Finally, we test the solution on the testing data:","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"final_model = train_model(ho.maximizer...);\naccuracy(pred(final_model, x_test), y_test)","category":"page"},{"location":"tools/hyperopt/","page":"Hyperopt.jl","title":"Hyperopt.jl","text":"This concludes a very simple example of how to integrate JsonGrinder.jl with Hyperopt.jl. Note that we could and should go further and experiment not only with the hyperparameters presented here, but also with the definition of the schema and/or the extractor, which can also have significant impact on the results.","category":"page"},{"location":"motivation/#Motivation","page":"Motivation","title":"Motivation","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Imagine that you want to train a model for processing hierarchical JSON documents. One example of a JSON document describing a molecule measurements may look like this:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"{\n    \"ind1\": 1,\n    \"inda\": 0,\n    \"logp\": 6.01,\n    \"lumo\": -2.184,\n    \"atoms\": [\n        {\n            \"element\": \"c\",\n            \"atom_type\": 22,\n            \"charge\": -0.118,\n            \"bonds\": [\n                { \"bond_type\": 7, \"element\": \"c\", \"atom_type\": 22, \"charge\": -0.118 },\n                { \"bond_type\": 1, \"element\": \"h\", \"atom_type\": 3, \"charge\": 0.141 },\n                { \"bond_type\": 7, \"element\": \"c\", \"atom_type\": 22, \"charge\": -0.118 }\n            ]\n        },\n        {\n            \"element\": \"c\",\n            \"atom_type\": 27,\n            \"charge\": 0.012,\n            \"bonds\": [\n                { \"bond_type\": 7, \"element\": \"c\", \"atom_type\": 22, \"charge\": -0.118 },\n                { \"bond_type\": 7, \"element\": \"c\", \"atom_type\": 27, \"charge\": -0.089 },\n                { \"bond_type\": 7, \"element\": \"c\", \"atom_type\": 22, \"charge\": -0.118 }\n            ]\n        }\n    ]\n}","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"We would like to predict the mutagenicity for Salmonella typhimurium of this molecule (for example, the example above is mutagenic).","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Majority of machine learning libraries assume the data takes form of tensors of a fixed dimension (like vectors or images) or a sequence of such tensors.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In contrast, JsonGrider.jl only requires your data to be stored in a flexible JSON format, and tries to automate most labor using reasonable defaults, while still giving you an option to control and tweak almost everything. JsonGrider.jl is built on top of Mill.jl which itself is built on top of Flux.jl.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"unk: Other formats\nAlthough JsonGrinder was designed for JSON files, it can easily be adapted for XML, Protocol Buffers, MessagePack, and other similar formats.","category":"page"},{"location":"motivation/#Pipeline-structure","page":"Motivation","title":"Pipeline structure","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Standard JsonGrinder.jl pipeline usually consists of five steps:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Create a schema of JSON files (using schema).\nFrom this schema create an extractor converting JSONs to Mill.jl structures (e.g. using suggestextractor).\nExtract your JSON documents into Mill.jl structures with the extractor (e.g. with extract). If all data fits into memory, extract everything at once, or extract on-demand when training.\nDefine a suitable model (e.g. using Mill.reflectinmodel).\nTrain the model, the library is 100% compatible with the Flux.jl tooling.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The basic workflow can be visualized as follows:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"<img class=\"display-light-only\" src=\"../assets/workflow.svg\" alt=\"JsonGrinder workflow\" style=\"width: 40%;\"/>\n<img class=\"display-dark-only\" src=\"../assets/workflow-dark.svg\" alt=\"JsonGrinder workflow\" style=\"width: 40%;\"/>","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The framework is able to process hierarchical JSON documents of any schema, embedding the documents into vectors. The embeddings can be used for classification, regression, and other ML tasks. Thanks to Mill.jl, models can handle missing values at all levels.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"See Mutagenesis for complete example of processing JSONs like the one above, including code.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"using JsonGrinder","category":"page"},{"location":"manual/extraction/#Extraction","page":"Extraction","title":"Extraction","text":"","category":"section"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Extractor is responsible for converting JSON documents into Mill.jl structures. The main idea is that the extractor follows the same hierarchical structure as previously inferred Schema. Extractor for a whole JSON is created by composing (sub-)extractors while reflecting the JSON structure.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Assume the following dataset of two JSON documents for which we infer a Schema:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"using JSON","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"jss = JSON.parse(\"\"\"[\n    {\n        \"name\": \"Karl\",\n        \"siblings\": [\"Gertruda\", \"Heike\", \"Fritz\"],\n        \"hobby\": [\"running\", \"pingpong\"],\n        \"age\": 21\n    },\n    {\n        \"name\": \"Heike\",\n        \"siblings\": [\"Gertruda\", \"Heike\", \"Fritz\"],\n        \"hobby\": [\"yoga\"],\n        \"age\": 24\n    }\n]\"\"\");\nsch = schema(jss)","category":"page"},{"location":"manual/extraction/#Manual-creation-of-[Extractor](@ref)s","page":"Extraction","title":"Manual creation of Extractors","text":"","category":"section"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"One possible way to create an Extractor is to manually define it from all the required pieces. One extractor corresponding to sch might look like this:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e = DictExtractor((\n    name = NGramExtractor(),\n    age = ScalarExtractor(),\n    hobby = ArrayExtractor(CategoricalExtractor([\"running\", \"swimming\",\"yoga\"]))\n))","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"We have just created a DictExtractor with","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"NGramExtractor to extract Strings under the \"name\" key,\nScalarExtractor to extract age under the \"age\" key, and finally\nArrayExtractor for extracting arrays under the \"hobby\" key. This extractor has one child, a CategoricalExtractor, which operates on three hobby categories.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Applying e on the first JSON document yields the following hierarchy of Mill.jl structures:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"x = e(jss[1])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"ukn: Consistent preprocessing\nIf any preprocessing was performed for input documents as for example discussed in Preprocessing make sure to apply the same preprocessing before passing documents to any Extractor as well!","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"ukn: Missing key\nNote that we didn't include any extractor for the \"siblings\" key. In such case, the key in the JSON document is simply ignored and never extracted.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Every (sub)extractor, a node in the extractor \"tree\" is also callable, for example:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e[:hobby](jss[1][\"hobby\"])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Let's inspect how the subtree under the \"hobby\" key in the JSON was extracted:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"printtree(x; trav=true)\nx[\"s\"]","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"The first column in the OneHotMatrix corresponds to \"running\", which is the first category in the corresponding CategoricalExtractor. The second column corresponds to \"pingpong\", which is an unknown category in the extractor. Any other unknown String would be extracted in the same way:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e[\"s\"](\"unknown\")","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"ukn: Further reading\nFor more information about individual subtypes of Extractor, see their docs, or Extractor API.","category":"page"},{"location":"manual/extraction/#Semi-automatic-[Extractor](@ref)-creation","page":"Extraction","title":"Semi-automatic Extractor creation","text":"","category":"section"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Manually creating Extractors is a laborous and error-prone process once the hierarchical structure of input JSON documents gets large. For this reason, JsonGrinder.jl provides the suggestextractor function greatly simplifying this process:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e = suggestextractor(sch)","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"The function uses a simple heuristic for choosing reasonable extractors for values in leaves: if there are not many unique values (less than the categorical_limit keyword argument), use CategoricalExtractor, else use either NGramExtractor or ScalarExtractor depending on the type.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"ukn: Hooking into the behavior\nIt is possible to hook into the internals of how suggestextractor treats values in leaves by redefining _suggestextractor(e::LeafEntry).","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Please refer to the suggestextractor docs for all possible keyword arguments.","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"ukn: Inspect the result\nIt is recommended to check the proposed extractor manually, and modifying it if it makes sense.","category":"page"},{"location":"manual/extraction/#Stable-[Extractor](@ref)s","page":"Extraction","title":"Stable Extractors","text":"","category":"section"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Sometimes not all JSON documents in a dataset are complete. For example:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"jss = JSON.parse.([\n    \"\"\" { \"a\" : 1, \"b\" : \"foo\" } \"\"\",\n    \"\"\" { \"b\" : \"bar\" } \"\"\"\n]);\nsch = schema(jss)","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"In such case, suggestextractor wraps the extractor corresponding to the key with missing data (\"a\") into StableExtractor:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e = suggestextractor(sch)","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"and the extraction works fine:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"extract(e, jss)","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"If the dataset for schema inference is undersampled and the missing key doesn't show up, suggestextractor will infer unsuitable Extractor:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"sch = schema(jss[1:1])\ne = suggestextractor(sch)\ne(jss[2])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"There are multiple ways to deal with this problem:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Manually wrap the problematic node (here with the help of  Accessors.jl):","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"using Accessors","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e_stable = @set e.children[:a] = StableExtractor(e[:a])\ne_stable(jss[2])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Use stabilizeextractor on the whole tree (or a subtree):","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e_stable = stabilizeextractor(e)\ne_stable(jss[2])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Call suggestextractor with all_stable=true. Now all document values are treated as possibly missing. Results of stabilizeextractor(schema(...)) and suggestextractor(...; all_stable=true) are roughly equivalent:","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"e_stable = suggestextractor(sch; all_stable=true)\ne_stable(jss[2])","category":"page"},{"location":"manual/extraction/","page":"Extraction","title":"Extraction","text":"Preprocess the data (delete the problematic key from all documents or the schema, or make sure  that documents with the missing key are present in the data when calling schema).","category":"page"},{"location":"api/extractor/#extractor_api","page":"Extractors","title":"Extractor API","text":"","category":"section"},{"location":"api/extractor/#Index","page":"Extractors","title":"Index","text":"","category":"section"},{"location":"api/extractor/","page":"Extractors","title":"Extractors","text":"Pages = [\"extractor.md\"]","category":"page"},{"location":"api/extractor/#API","page":"Extractors","title":"API","text":"","category":"section"},{"location":"api/extractor/","page":"Extractors","title":"Extractors","text":"suggestextractor\nstabilizeextractor\nextract\n\nExtractor\n\nLeafExtractor\nStableExtractor\n\nScalarExtractor\nCategoricalExtractor\nNGramExtractor\n\nDictExtractor\nArrayExtractor\nPolymorphExtractor","category":"page"},{"location":"api/extractor/#JsonGrinder.suggestextractor","page":"Extractors","title":"JsonGrinder.suggestextractor","text":"suggestextractor(e::Schema; min_occurences=1, all_stable=false, categorical_limit=100)\n\ngiven schema e, create a corresponding Extractor\n\nmin_occurences specifies the minimum occurence of a key to be included in the extractor.\nall_stable makes all leaf extractors strictly stable.\ncategorical_limit specifies the maximum number of different values in a leaf for it to be   considered a categorical variable.\nngram_params makes it possible to override default params for NGramExtractor.\n\nExamples\n\njulia> s = schema([ Dict(\"a\" => 1, \"b\" => [\"foo\"], \"c\" => Dict(\"d\" => 1)),\n                    Dict(\"a\" => 2,                 \"c\" => Dict())])\nDictEntry 2x updated\n  ├── a: LeafEntry (2 unique `Real` values) 2x updated\n  ├── b: ArrayEntry 1x updated\n  │        ╰── LeafEntry (1 unique `String` values) 1x updated\n  ╰── c: DictEntry 2x updated\n           ╰── d: LeafEntry (1 unique `Real` values) 1x updated\n\njulia> suggestextractor(s)\nDictExtractor\n  ├── a: CategoricalExtractor(n=3)\n  ├── b: ArrayExtractor\n  │        ╰── StableExtractor(CategoricalExtractor(n=2))\n  ╰── c: DictExtractor\n           ╰── d: StableExtractor(CategoricalExtractor(n=2))\n\njulia> suggestextractor(s; all_stable=true)\nDictExtractor\n  ├── a: StableExtractor(CategoricalExtractor(n=3))\n  ├── b: ArrayExtractor\n  │        ╰── StableExtractor(CategoricalExtractor(n=2))\n  ╰── c: DictExtractor\n           ╰── d: StableExtractor(CategoricalExtractor(n=2))\n\njulia> suggestextractor(s; min_occurences=2)\nDictExtractor\n  ╰── a: CategoricalExtractor(n=3)\n\njulia> suggestextractor(s; categorical_limit=0)\nDictExtractor\n  ├── a: ScalarExtractor(c=1.0, s=1.0)\n  ├── b: ArrayExtractor\n  │        ╰── StableExtractor(NGramExtractor(n=3, b=256, m=2053))\n  ╰── c: DictExtractor\n           ╰── d: StableExtractor(ScalarExtractor(c=1.0, s=1.0))\n\nSee also: extract, stabilizeextractor.\n\n\n\n\n\n","category":"function"},{"location":"api/extractor/#JsonGrinder.stabilizeextractor","page":"Extractors","title":"JsonGrinder.stabilizeextractor","text":"stabilizeextractor(e::Extractor)\n\nReturns a new extractor with similar structure as e, containing StableExtractor in its leaves.\n\nExamples\n\njulia> e = (a=ScalarExtractor(), b=CategoricalExtractor(1:5)) |> DictExtractor\nDictExtractor\n  ├── a: ScalarExtractor(c=0.0, s=1.0)\n  ╰── b: CategoricalExtractor(n=6)\n\njulia> e_stable = stabilizeextractor(e)\nDictExtractor\n  ├── a: StableExtractor(ScalarExtractor(c=0.0, s=1.0))\n  ╰── b: StableExtractor(CategoricalExtractor(n=6))\n\njulia> e(Dict(\"a\" => 0))\nERROR: IncompatibleExtractor at path [:b]: this path contains missing data not supported by this extractor! See the `Stable Extractors` section in the docs.\n[...]\n\njulia> e_stable(Dict(\"a\" => 0))\nProductNode  1 obs\n  ├── a: ArrayNode(1×1 Array with Union{Missing, Float32} elements)  1 obs\n  ╰── b: ArrayNode(6×1 MaybeHotMatrix with Union{Missing, Bool} elements)  1 obs\n\nSee also: suggestextractor, extract.\n\n\n\n\n\n","category":"function"},{"location":"api/extractor/#JsonGrinder.extract","page":"Extractors","title":"JsonGrinder.extract","text":"extract(e::Extractor, samples; store_input=Val(false))\n\nEfficient extraction of multiple samples at once.\n\nNote that whereas extract expects samples to be an iterable of samples (of known length), calling the extractor directly with e(sample) works for a single sample. In other words, e(sample) is equivalent to extract(e, [sample]).\n\nSee also: suggestextractor, stabilizeextractor, schema.\n\nExamples\n\njulia> sample = Dict(\"a\" => 0, \"b\" => \"foo\");\n\njulia> e = suggestextractor(schema([sample]))\nDictExtractor\n  ├── a: CategoricalExtractor(n=2)\n  ╰── b: CategoricalExtractor(n=2)\n\njulia> e(sample)\nProductNode  1 obs\n  ├── a: ArrayNode(2×1 OneHotArray with Bool elements)  1 obs\n  ╰── b: ArrayNode(2×1 OneHotArray with Bool elements)  1 obs\n\njulia> e(sample) == extract(e, [sample])\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/extractor/#JsonGrinder.Extractor","page":"Extractors","title":"JsonGrinder.Extractor","text":"Extractor\n\nSupertype for all extractor node types.\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.LeafExtractor","page":"Extractors","title":"JsonGrinder.LeafExtractor","text":"LeafExtractor\n\nSupertype for all leaf extractor node types that reside in the leafs of the hierarchy.\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.StableExtractor","page":"Extractors","title":"JsonGrinder.StableExtractor","text":"struct StableExtractor{T <: LeafExtractor} <: LeafExtractor\n\nWraps any other LeafExtractor and makes it output stable results w.r.t. missing input values.\n\nSee also: stabilizeextractor.\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.ScalarExtractor","page":"Extractors","title":"JsonGrinder.ScalarExtractor","text":"ScalarExtractor{T} <: Extractor\n\nExtracts a numerical value, centered by subtracting c and scaled by s.\n\nExamples\n\njulia> e = ScalarExtractor(2, 3)\nScalarExtractor(c=2.0, s=3.0)\n\njulia> e(0)\n1×1 ArrayNode{Matrix{Float32}, Nothing}:\n -6.0\n\njulia> e(1)\n1×1 ArrayNode{Matrix{Float32}, Nothing}:\n -3.0\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.CategoricalExtractor","page":"Extractors","title":"JsonGrinder.CategoricalExtractor","text":"CategoricalExtractor{V, I} <: Extractor\n\nExtracts a single item interpreted as a categorical variable into a one-hot encoded vector.\n\nThere is always an extra category for an unknown value (and hence the displayed n is one more than the number of categories).\n\nExamples\n\njulia> e = CategoricalExtractor(1:3)\nCategoricalExtractor(n=4)\n\njulia> e(2)\n4×1 ArrayNode{OneHotMatrix{UInt32, Vector{UInt32}}, Nothing}:\n ⋅\n 1\n ⋅\n ⋅\n\njulia> e(-1)\n4×1 ArrayNode{OneHotMatrix{UInt32, Vector{UInt32}}, Nothing}:\n ⋅\n ⋅\n ⋅\n 1\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.NGramExtractor","page":"Extractors","title":"JsonGrinder.NGramExtractor","text":"NGramExtractor{T} <: Extractor\n\nExtracts String as n-grams (Mill.NGramMatrix).\n\nExamples\n\njulia> e = NGramExtractor()\nNGramExtractor(n=3, b=256, m=2053)\n\njulia> e(\"foo\")\n2053×1 ArrayNode{NGramMatrix{String, Vector{String}, Int64}, Nothing}:\n \"foo\"\n\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.DictExtractor","page":"Extractors","title":"JsonGrinder.DictExtractor","text":"DictExtractor{S} <: Extractor\n\nExtracts all items in a Dict and returns them as a Mill.ProductNode.\n\nExamples\n\njulia> e = (a=ScalarExtractor(), b=CategoricalExtractor(1:5)) |> DictExtractor\nDictExtractor\n  ├── a: ScalarExtractor(c=0.0, s=1.0)\n  ╰── b: CategoricalExtractor(n=6)\n\njulia> e(Dict(\"a\" => 1, \"b\" => 1))\nProductNode  1 obs\n  ├── a: ArrayNode(1×1 Array with Float32 elements)  1 obs\n  ╰── b: ArrayNode(6×1 OneHotArray with Bool elements)  1 obs\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.ArrayExtractor","page":"Extractors","title":"JsonGrinder.ArrayExtractor","text":"ArrayExtractor{T}\n\nExtracts all items in an Array and returns them as a Mill.BagNode.\n\nExamples\n\njulia> e = ArrayExtractor(CategoricalExtractor(2:4))\nArrayExtractor\n  ╰── CategoricalExtractor(n=4)\n\njulia> e([2, 3, 1, 4])\nBagNode  1 obs\n  ╰── ArrayNode(4×4 OneHotArray with Bool elements)  4 obs\n\n\n\n\n\n","category":"type"},{"location":"api/extractor/#JsonGrinder.PolymorphExtractor","page":"Extractors","title":"JsonGrinder.PolymorphExtractor","text":"PolymorphExtractor\n\nExtracts to a Mill.ProductNode where each item is a result of different extractor.\n\nExamples\n\njulia> e = (NGramExtractor(), CategoricalExtractor([\"tcp\", \"udp\", \"dhcp\"])) |> PolymorphExtractor\nPolymorphExtractor\n  ├── NGramExtractor(n=3, b=256, m=2053)\n  ╰── CategoricalExtractor(n=4)\n\njulia> e(\"tcp\")\nProductNode  1 obs\n  ├── ArrayNode(2053×1 NGramMatrix with Int64 elements)  1 obs\n  ╰── ArrayNode(4×1 OneHotArray with Bool elements)  1 obs\n\njulia> e(\"http\")\nProductNode  1 obs\n  ├── ArrayNode(2053×1 NGramMatrix with Int64 elements)  1 obs\n  ╰── ArrayNode(4×1 OneHotArray with Bool elements)  1 obs\n\n\n\n\n\n","category":"type"},{"location":"citation/#Citation","page":"Citation","title":"Citation","text":"","category":"section"},{"location":"citation/","page":"Citation","title":"Citation","text":"Kindly cite our work with the following entries if you find it interesting, please:","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"JsonGrinder.jl: automated differentiable neural architecture for embedding arbitrary JSON data\n@article{Mandlik2021,\n  author  = {Šimon Mandlík and Matěj Račinský and Viliam Lisý and Tomáš Pevný},\n  title   = {JsonGrinder.jl: automated differentiable neural architecture for embedding arbitrary JSON data},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {298},\n  pages   = {1--5},\n  url     = {http://jmlr.org/papers/v23/21-0174.html}\n}\nMalicious Internet Entity Detection Using Local Graph Inference (practical Mill.jl and JsonGrinder.jl application)\n@article{Mandlik2024,\n  author  = {Mandlík, Šimon and Pevný, Tomáš and Šmídl, Václav and Bajer, Lukáš},\n  journal = {IEEE Transactions on Information Forensics and Security},\n  title   = {Malicious Internet Entity Detection Using Local Graph Inference},\n  year    = {2024},\n  volume  = {19},\n  pages   = {3554-3566},\n  doi     = {10.1109/TIFS.2024.3360867}\n}\nthis implementation (fill in the used version)\n@software{JsonGrinder,\n  author  = {Tomas Pevny and Matej Racinsky and Simon Mandlik},\n  title   = {JsonGrinder.jl: a flexible library for automated feature engineering and conversion of JSONs to Mill.jl structures},\n  url     = {https://github.com/CTUAvastLab/JsonGrinder.jl},\n  version = {...},\n}","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"using Pkg\nold_path = Pkg.project().path\nPkg.activate(pwd())\nPkg.instantiate()","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"EditURL = \"recipes_literate.jl\"","category":"page"},{"location":"examples/recipes/recipes/#Recipe-Ingredients","page":"Recipes","title":"Recipe Ingredients","text":"","category":"section"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"The following simple example shows how to train a hierarchical model for predicting the type of cuisine from a set of used ingredients.","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"ukn: Jupyter notebook\nThis example is also available as a Jupyter notebook and the environment and the data are accessible here.","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"ukn: Recommended reading\nWe recommend to first read the Mutagenesis example, which introduces core concepts. This example shows application on another dataset and integration with JSON3.jl.","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We load all dependencies and fix the seed:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"using JsonGrinder, Mill, Flux, OneHotArrays, JSON3, MLUtils, Statistics\n\nusing Random; Random.seed!(42);\nnothing #hide","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"The full dataset and the problem description can be also found on Kaggle, but for demonstration purposes we load only its small subset:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"dataset = JSON3.read.(readlines(\"recipes.jsonl\"));\nshuffle!(dataset);\njss_train, jss_test = dataset[1:2000], dataset[2001:end];\njss_train[1]","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Labels are stored in the \"cuisine\" field:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"y_train = getindex.(jss_train, \"cuisine\");\ny_test = getindex.(jss_test, \"cuisine\");\ny_train","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"In this example we have more classes than two, so we also encode all training labels into one-hot vectors:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"classes = unique(y_train)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"y_train_oh = onehotbatch(y_train, classes)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Now we create a schema:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"sch = schema(jss_train)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"ukn: Function as first argument\nFunction schema accepts an optional argument, a function first mapping all elements of an input array. We could thus reduce the schema creation into a single command schema(JSON3.read, readlines(\"recipes.jsonl\")).","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"From the schema, we will delete the \"cuisine\" key storing the label, and also the \"id\" key, which is just the id of the sample, which is not useful in training:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"delete!(sch.children, :cuisine);\ndelete!(sch.children, :id);\nsch","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We can see that only a single key \"ingredients\" is left. We can thus just take its content:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"jss_train = getindex.(jss_train, \"ingredients\");\njss_test = getindex.(jss_test, \"ingredients\");\njss_train[1]","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We can infer the schema again, or just take a subtree of the original schema","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We can just take the only subtree of the original schema sch:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"sch[:ingredients]","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Or infer it once again, this time jss_train is not a Vector of Dicts, but a Vector of Vectors:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"sch = schema(jss_train)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Next step is to create an extractor:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"e = suggestextractor(sch)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"If we have sufficient memory, we can extract all documents before training like in the Mutagenesis example:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"extract(e, jss_train)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"However, in this example we want to show how to extract online in the training loop.","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We continue with the model definition, making use of some of the Mill.reflectinmodel features. We continue with the model definition, making use of some of the","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"encoder = reflectinmodel(sch, e, d -> Dense(d, 40, relu), d -> SegmentedMeanMaxLSE(d) |> BagCount)\nmodel = Dense(40, length(classes)) ∘ encoder","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"We define important components for the training:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"pred(m, x) = softmax(m(x))\nopt_state = Flux.setup(Flux.Optimise.Adam(), model);\nminibatch_iterator = Flux.DataLoader((jss_train, y_train_oh), batchsize=32, shuffle=true);\naccuracy(p, y) = mean(onecold(p, classes) .== y)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"And run the training:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"for i in 1:20\n    Flux.train!(model, minibatch_iterator, opt_state) do m, jss, y\n        x = Flux.@ignore_derivatives extract(e, jss)\n        Flux.Losses.logitcrossentropy(m(x), y)\n    end\n    @info \"Epoch $i\" accuracy=accuracy(pred(model, extract(e, jss_train)), y_train)\nend","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Finally, let's measure the testing accuracy. In this case, the classifier is overfitted:","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"accuracy(model(extract(e, jss_test)), y_test)","category":"page"},{"location":"examples/recipes/recipes/","page":"Recipes","title":"Recipes","text":"Pkg.activate(old_path)","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img class=\"display-light-only\" src=\"assets/logo.svg\" alt=\"JsonGrinder.jl logo\" style=\"width: 70%;\"/>\n<img class=\"display-dark-only\" src=\"assets/logo-dark.svg\" alt=\"JsonGrinder.jl logo\" /style=\"width: 70%;\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"JsonGrinder.jl is a library that facilitates processing of JSON documents into Mill.jl structures for machine learning. It provides functionality for JSON schema inference, extraction of JSON documents to a suitable representation for machine learning, and constructing a model operating on this data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Watch our introductory talk from JuliaCon 2021.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Run the following in REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add JsonGrinder","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia v1.10 or later is required.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For the quickest start, see the Mutagenesis example.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Motivation: a brief introduction and motivation\nManual: a tutorial about the package\nExamples: a collection of examples\nExternal tools: examples of integration with other packages\nPublic API: extensive API reference\nCitation: preferred citation entries\nMill.jl: a core dependence of the package","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"using JsonGrinder","category":"page"},{"location":"manual/schema_inference/#manual_schema","page":"Schema inference","title":"Schema inference","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"The schema helps to understand the structure of JSON documents and stores some basic statistics of values present in the dataset. All this information is later taken into the account in the suggestextractor function, which takes a schema and using few reasonable heuristic, suggests a suitable extractor for converting JSONs to Mill.jl structures.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"In this simple example, we start with a dataset of three JSON documents:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"using JSON","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"jss = JSON.parse.([\n   \"\"\"{ \"a\": \"Hello\", \"b\": { \"c\": 1, \"d\": [] } }\"\"\",\n   \"\"\"{ \"b\": { \"c\": 1, \"d\": [1, 2, 3] } }\"\"\",\n   \"\"\"{ \"a\": \"World\", \"b\": { \"c\": 2, \"d\": [1, 3] } }\"\"\",\n]);\njss[1]","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"The main function for creating schema is schema, which accepts an array documents and produces a Schema:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"sch = schema(jss[1:2])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"This schema might already come handy for quick statistical insight into the dataset at hand, which we discuss further below.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Schema can be always updated with another document with the update! function:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"update!(sch, jss[3]);\nsch","category":"page"},{"location":"manual/schema_inference/#Schema-merging","page":"Schema inference","title":"Schema merging","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Lastly, it is also possible to merge two or more schemas together with Base.merge and Base.merge!. It is thus possible to easily parallelize schema inference, merging together individual (sub)schemas as follows:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"sch1 = schema(jss[1:2])\nsch2 = schema(jss[3:3])\nmerge(sch1, sch2)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"or inplace merge into sch1:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"merge!(sch1, sch2);\nsch1","category":"page"},{"location":"manual/schema_inference/#Advanced","page":"Schema inference","title":"Advanced","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Statistics are collected in a hierarchical structure reflecting the structured composed of DictEntry, ArrayEntry, and LeafEntry. These structures are direct counterparts to those in JSON: Dict, Array, and Value.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"In this example we will load larger dataset of JSON documents (available here):","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"jss = JSON.parsefile(\"json_examples.json\");\njss[1]","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"and compute the schema:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"sch = schema(jss)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"To save space, hierarchical structures like schema and extractor are not shown in full in REPL. To inspect the full schema, we can use printtree from HierarchicalUtils.jl. We use","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"vtrunc=3, which only shows at most 3 children of each node in the tree, and\ntrav=true, which also shows traversal codes of individual nodes.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"printtree(sch; vtrunc=3, trav=true)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Traversal codes (strings printed at the end of rows) can be used to access individual elements of the schema.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"sch[\"JQ\"]\nsch[\"sc\"]","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"As indicated in the displayed tree, LeafEntry accessible as sch[\"JQ\"] was updated 3 times in input documents. On the other hand, LeafEntry accessible as sch[\"sc\"] was updated 12 times, each time with a different value.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"ukn: Empty arrays\nNote that for example the ArrayEntry accessible as sch[\"AU\"] has been updated 22 times, but doesn't have any children. This is because on path with keys \"abstract\" and \"cite_spans\",  we have seen 22 arrays, but all were empty.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"To learn more about the HierarchicalUtils.jl package, check also this section of docs, or this section in the Mill.jl docs.","category":"page"},{"location":"manual/schema_inference/#Schema-parameters","page":"Schema inference","title":"Schema parameters","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"It may happen that values in leaves of the documents are too unique. Saving all values might quickly become too memory demanding. JsonGrinder.jl thus works with JsonGrinder.max_values parameter. Once the number of unique values in one leaf exceeds this parameter, schema will no longer remember new appearing values in this leaf. This behavior might be relevant when calling suggestextractor, especially with the categorical_limit argument.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Similarly, JsonGrinder.jl also shortens strings that are too long before saving them to schema. This can be governed with the JsonGrinder.max_string_codeunits parameter.","category":"page"},{"location":"manual/schema_inference/#Preprocessing","page":"Schema inference","title":"Preprocessing","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Sometimes, input JSON documents do not adhere to a stable schema, which for example happens if one key has children of multiple different types in different documents. An example would be:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"jss = JSON.parse.([\n    \"\"\" {\"a\": [1, 2, 3] } \"\"\",\n    \"\"\" {\"a\": { \"b\": 1 } } \"\"\",\n    \"\"\" {\"a\": \"hello\" } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"In these cases the schema creation fails indicating what went wrong:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(jss)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Should this happen, we recommend to deal with such cases by suitable preprocessing.","category":"page"},{"location":"manual/schema_inference/#Mapping-paths","page":"Schema inference","title":"Mapping paths","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Assume that input documents contain information about port numbers, some of which are encoded as integers and some of which as strings:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"jss = [\n    \"\"\" {\"ports\": [70, 80, 443], \"protocol\": \"TCP\" } \"\"\",\n    \"\"\" {\"ports\": [\"22\", \"80\", \"500\"], \"protocol\": \"UDP\" } \"\"\",\n]","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(JSON.parse, jss)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"We recommend to deal with these cases using optic approach from Accessors.jl (and possibly also from AccessorsExtra.jl). We can use Accessors.modify to modify the problematic paths, turning all into Strings:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"using Accessors","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"f = js -> Accessors.modify(string, js, @optic _[\"ports\"][∗])\nf.(JSON.parse.(jss))\nschema(f ∘ JSON.parse, jss)","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"or parsing them as Integers:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(jss) do doc\n    js = JSON.parse(doc)\n    Accessors.modify(x -> x isa Integer ? x : parse(Int, x), js, @optic _[\"ports\"][∗])\nend","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"ukn: Writing `∗`\nAsterisk for selecting all elements of the array (∗) is not the standard star (*), but is written as \\ast<TAB> in Julia REPL, see also Accessors.jl docstrings.","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"We can also get rid of this path completely with Accessors.delete:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(jss) do doc\n    Accessors.delete(JSON.parse(doc), @optic _[\"ports\"])\nend","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"If JSON3 is used for parsing, it uses Symbols for keys in objects instead of Strings so make sure to use Symbols:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"using JSON3","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"Accessors.delete(JSON3.read(\"\"\" {\"port\": 1} \"\"\"), @optic _[\"port\"])\nAccessors.delete(JSON3.read(\"\"\" {\"port\": 1} \"\"\"), @optic _[:port])","category":"page"},{"location":"manual/schema_inference/#Null-values","page":"Schema inference","title":"Null values","text":"","category":"section"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"In the current version, JsonGrinder.jl does not support null values in JSON documents (represented as nothing in Julia):","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(JSON.parse, [\n    \"\"\" {\"a\": null } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(JSON.parse, [\n    \"\"\" {\"a\": [1, null, 3] } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(JSON.parse, [\n    \"\"\" {\"a\": {\"b\": null} } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"These values usually do not carry any relevant information, therefore, as the error suggests, the most straighforward and easiest solution is to filter them out using remove_nulls function:","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(remove_nulls ∘ JSON.parse, [\n    \"\"\" {\"a\": null } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(remove_nulls ∘ JSON.parse, [\n    \"\"\" {\"a\": [1, null, 3] } \"\"\"\n])","category":"page"},{"location":"manual/schema_inference/","page":"Schema inference","title":"Schema inference","text":"schema(remove_nulls ∘ JSON.parse, [\n    \"\"\" {\"a\": {\"b\": null} } \"\"\"\n])","category":"page"}]
}
