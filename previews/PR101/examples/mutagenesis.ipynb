{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mutagenesis Example\n",
    "Following example demonstrates learning to [predict the mutagenicity on Salmonella typhimurium](https://relational.fit.cvut.cz/dataset/Mutagenesis) (dataset is stored in json format [in MLDatasets.jl](https://juliaml.github.io/MLDatasets.jl/stable/datasets/Mutagenesis/) for your convenience)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Installing scipy via the Conda scipy package...\n",
      "[ Info: Running `conda install -q -y scipy` in root environment\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/runner/.julia/conda/3\n",
      "\n",
      "  added / updated specs:\n",
      "    - scipy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB\n",
      "    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB\n",
      "    scipy-1.7.1                |   py39h292c36d_2        16.9 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        17.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.5.0-ha8ba4b0_17\n",
      "  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17\n",
      "  scipy              pkgs/main/linux-64::scipy-1.7.1-py39h292c36d_2\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLDatasets, JsonGrinder, Flux, Mill, MLDataPattern, Statistics, ChainRulesCore"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "start by loading all samples"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "train_x, train_y = MLDatasets.Mutagenesis.traindata();\n",
    "test_x, test_y = MLDatasets.Mutagenesis.testdata();\n",
    "minibatchsize = 100\n",
    "iterations = 5_000\n",
    "neurons = 20 \t\t# neurons per layer"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    " Create the schema and extractor from training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mDict\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mCategorical d = 99\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mCategorical d = 2\n\u001b[34m  ⋮\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mArray of\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mDict\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "sch = JsonGrinder.schema(train_x)\n",
    "extractor = suggestextractor(sch)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    " Convert samples to Mill structure and extract targets"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data[1] = ProductNode\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mProductNode\u001b[39m\u001b[90m \t# 1 obs, 104 bytes\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mArrayNode(99×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mArrayNode(2×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── logp: \u001b[39m\u001b[39mArrayNode(63×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── ind1: \u001b[39m\u001b[39mArrayNode(3×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mBagNode\u001b[39m\u001b[90m \t# 1 obs, 136 bytes\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mProductNode\u001b[39m\u001b[90m \t# 26 obs, 64 bytes\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "train_data = extractor.(train_x)\n",
    "test_data = extractor.(test_x)\n",
    "labelnames = unique(train_y)\n",
    "\n",
    "@show train_data[1]"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mProductModel ↦ ArrayModel(Dense(100, 2))\u001b[39m\u001b[90m \t# 2 arrays, 202 params, 888 bytes\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mArrayModel(Dense(99, 20, relu))\u001b[90m \t# 2 arrays, 2_000 params, 7.891 KiB\u001b[39m\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mArrayModel(Dense(2, 20, relu))\u001b[90m \t# 2 arrays, 60 params, 320 bytes\u001b[39m\n\u001b[34m  ├─── logp: \u001b[39m\u001b[39mArrayModel(Dense(63, 20, relu))\u001b[90m \t# 2 arrays, 1_280 params, 5.078 KiB\u001b[39m\n\u001b[34m  ├─── ind1: \u001b[39m\u001b[39mArrayModel(Dense(3, 20, relu))\u001b[90m \t# 2 arrays, 80 params, 400 bytes\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mBagModel ↦ [SegmentedMean(20); SegmentedMax(20)] ↦ ArrayModel(Dense(40, 20, relu))\u001b[39m\u001b[90m \t# 4 arrays, 860 params, 3.516 KiB\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mProductModel ↦ ArrayModel(Dense(61, 20, relu))\u001b[39m\u001b[90m \t# 2 arrays, 1_240 params, 4.922 KiB\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "model = reflectinmodel(sch, extractor,\n",
    "\tlayer -> Dense(layer, neurons, relu),\n",
    "\tbag -> SegmentedMeanMax(bag),\n",
    "\tfsm = Dict(\"\" => layer -> Dense(layer, length(labelnames))),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "let's define loss and some helper functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "#9 (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "loss(x,y) = Flux.logitcrossentropy(inference(x), Flux.onehotbatch(y, labelnames))\n",
    "inference(x::AbstractMillNode) = model(x).data\n",
    "inference(x::AbstractVector{<:AbstractMillNode}) = inference(reduce(catobs, x))\n",
    "accuracy(x,y) = mean(labelnames[Flux.onecold(inference(x))] .== y)\n",
    "loss(xy::Tuple) = loss(xy...)\n",
    "@non_differentiable Base.reduce(catobs, x::AbstractVector{<:AbstractMillNode})\n",
    "cb = () -> begin\n",
    "\ttrain_acc = accuracy(train_data, train_y)\n",
    "\ttest_acc = accuracy(test_data, test_y)\n",
    "\tprintln(\"accuracy: train = $train_acc, test = $test_acc\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "create minibatches"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: train = 0.4, test = 0.29545454545454547\n",
      "accuracy: train = 0.82, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.88, test = 0.8863636363636364\n",
      "accuracy: train = 0.9, test = 0.8863636363636364\n",
      "accuracy: train = 0.9, test = 0.8863636363636364\n",
      "accuracy: train = 0.91, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8636363636363636\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "minibatches = RandomBatches((train_data, train_y), size = minibatchsize, count = iterations)\n",
    "Flux.Optimise.train!(loss, Flux.params(model), minibatches, ADAM(), cb = Flux.throttle(cb, 2))"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classify test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8636363636363636"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "probs = softmax(inference(test_data))\n",
    "o = Flux.onecold(probs)\n",
    "pred_classes = labelnames[o]\n",
    "print(mean(pred_classes .== test_y))"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "we see the accuracy is around 79% on test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "#predicted classes for test set\n",
    "print(pred_classes)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "gt classes for test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "print(test_y)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "probabilities for test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.7966703 0.06769236 0.7126265 0.051600363 0.06066731 0.7419391 0.04184219 0.14291018 0.9724018 0.80198383 0.70539814 0.04861571 0.049208514 0.79428434 0.7124812 0.14294115 0.7840662 0.049588382 0.052255884 0.14241649 0.0067686387 0.96804017 0.7354195 0.990464 0.17703372 0.82597524 0.4959888 0.076788634 0.6863598 0.67669773 0.72284144 0.056862473 0.17945196 0.058966007 0.794795 0.7890754 0.905496 0.7967262 0.9926622 0.06749877 0.8608849 0.71532446 0.13802122 0.8161699; 0.20332964 0.93230766 0.28737348 0.9483996 0.93933266 0.25806087 0.9581578 0.8570898 0.027598204 0.19801615 0.2946019 0.95138425 0.9507915 0.20571566 0.28751886 0.8570588 0.2159338 0.9504117 0.9477441 0.8575836 0.9932314 0.031959876 0.2645805 0.00953606 0.8229663 0.1740247 0.5040112 0.9232114 0.3136402 0.32330227 0.27715862 0.9431376 0.82054806 0.94103396 0.20520507 0.21092467 0.09450401 0.2032737 0.007337767 0.93250126 0.13911508 0.2846755 0.8619788 0.18383014]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "print(probs)"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
