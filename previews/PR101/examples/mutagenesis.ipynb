{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mutagenesis Example\n",
    "Following example demonstrates learning to [predict the mutagenicity on Salmonella typhimurium](https://relational.fit.cvut.cz/dataset/Mutagenesis) (dataset is stored in json format [in MLDatasets.jl](https://juliaml.github.io/MLDatasets.jl/stable/datasets/Mutagenesis/) for your convenience)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Installing scipy via the Conda scipy package...\n",
      "[ Info: Running `conda install -q -y scipy` in root environment\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/runner/.julia/conda/3\n",
      "\n",
      "  added / updated specs:\n",
      "    - scipy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB\n",
      "    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB\n",
      "    scipy-1.7.1                |   py39h292c36d_2        16.9 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        17.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.5.0-ha8ba4b0_17\n",
      "  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17\n",
      "  scipy              pkgs/main/linux-64::scipy-1.7.1-py39h292c36d_2\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLDatasets, JsonGrinder, Flux, Mill, MLDataPattern, Statistics, ChainRulesCore"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "start by loading all samples"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "train_x, train_y = MLDatasets.Mutagenesis.traindata();\n",
    "test_x, test_y = MLDatasets.Mutagenesis.testdata();\n",
    "minibatchsize = 100\n",
    "iterations = 5_000\n",
    "neurons = 20 \t\t# neurons per layer"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    " Create the schema and extractor from training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mDict\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mCategorical d = 99\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mCategorical d = 2\n\u001b[34m  ⋮\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mArray of\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mDict\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "sch = JsonGrinder.schema(train_x)\n",
    "extractor = suggestextractor(sch)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    " Convert samples to Mill structure and extract targets"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data[1] = ProductNode\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mProductNode\u001b[39m\u001b[90m \t# 1 obs, 104 bytes\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mArrayNode(99×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mArrayNode(2×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── logp: \u001b[39m\u001b[39mArrayNode(63×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  ├─── ind1: \u001b[39m\u001b[39mArrayNode(3×1 OneHotArray with Bool elements)\u001b[90m \t# 1 obs, 60 bytes\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mBagNode\u001b[39m\u001b[90m \t# 1 obs, 136 bytes\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mProductNode\u001b[39m\u001b[90m \t# 26 obs, 64 bytes\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "train_data = extractor.(train_x)\n",
    "test_data = extractor.(test_x)\n",
    "labelnames = unique(train_y)\n",
    "\n",
    "@show train_data[1]"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[34mProductModel ↦ ArrayModel(Dense(100, 2))\u001b[39m\u001b[90m \t# 2 arrays, 202 params, 888 bytes\u001b[39m\n\u001b[34m  ├─── lumo: \u001b[39m\u001b[39mArrayModel(Dense(99, 20, relu))\u001b[90m \t# 2 arrays, 2_000 params, 7.891 KiB\u001b[39m\n\u001b[34m  ├─── inda: \u001b[39m\u001b[39mArrayModel(Dense(2, 20, relu))\u001b[90m \t# 2 arrays, 60 params, 320 bytes\u001b[39m\n\u001b[34m  ├─── logp: \u001b[39m\u001b[39mArrayModel(Dense(63, 20, relu))\u001b[90m \t# 2 arrays, 1_280 params, 5.078 KiB\u001b[39m\n\u001b[34m  ├─── ind1: \u001b[39m\u001b[39mArrayModel(Dense(3, 20, relu))\u001b[90m \t# 2 arrays, 80 params, 400 bytes\u001b[39m\n\u001b[34m  └── atoms: \u001b[39m\u001b[31mBagModel ↦ [SegmentedMean(20); SegmentedMax(20)] ↦ ArrayModel(Dense(40, 20, relu))\u001b[39m\u001b[90m \t# 4 arrays, 860 params, 3.516 KiB\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m  └── \u001b[39m\u001b[32mProductModel ↦ ArrayModel(Dense(61, 20, relu))\u001b[39m\u001b[90m \t# 2 arrays, 1_240 params, 4.922 KiB\u001b[39m\n\u001b[34m             \u001b[39m\u001b[31m      \u001b[39m\u001b[32m  ⋮\u001b[39m"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "model = reflectinmodel(sch, extractor,\n",
    "\tlayer -> Dense(layer, neurons, relu),\n",
    "\tbag -> SegmentedMeanMax(bag),\n",
    "\tfsm = Dict(\"\" => layer -> Dense(layer, length(labelnames))),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "let's define loss and some helper functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "#9 (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "loss(x,y) = Flux.logitcrossentropy(inference(x), Flux.onehotbatch(y, labelnames))\n",
    "inference(x::AbstractMillNode) = model(x).data\n",
    "inference(x::AbstractVector{<:AbstractMillNode}) = inference(reduce(catobs, x))\n",
    "accuracy(x,y) = mean(labelnames[Flux.onecold(inference(x))] .== y)\n",
    "loss(xy::Tuple) = loss(xy...)\n",
    "@non_differentiable Base.reduce(catobs, x::AbstractVector{<:AbstractMillNode})\n",
    "cb = () -> begin\n",
    "\ttrain_acc = accuracy(train_data, train_y)\n",
    "\ttest_acc = accuracy(test_data, test_y)\n",
    "\tprintln(\"accuracy: train = $train_acc, test = $test_acc\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "create minibatches"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: train = 0.42, test = 0.5227272727272727\n",
      "accuracy: train = 0.62, test = 0.6818181818181818\n",
      "accuracy: train = 0.78, test = 0.7727272727272727\n",
      "accuracy: train = 0.84, test = 0.8636363636363636\n",
      "accuracy: train = 0.85, test = 0.8636363636363636\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.84, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.85, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.86, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.88, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.87, test = 0.8863636363636364\n",
      "accuracy: train = 0.88, test = 0.8863636363636364\n",
      "accuracy: train = 0.89, test = 0.8863636363636364\n",
      "accuracy: train = 0.89, test = 0.8863636363636364\n",
      "accuracy: train = 0.91, test = 0.8863636363636364\n",
      "accuracy: train = 0.9, test = 0.8863636363636364\n",
      "accuracy: train = 0.91, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.92, test = 0.8863636363636364\n",
      "accuracy: train = 0.94, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n",
      "accuracy: train = 0.93, test = 0.8863636363636364\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "minibatches = RandomBatches((train_data, train_y), size = minibatchsize, count = iterations)\n",
    "Flux.Optimise.train!(loss, Flux.params(model), minibatches, ADAM(), cb = Flux.throttle(cb, 2))"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classify test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863636363636364"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "probs = softmax(inference(test_data))\n",
    "o = Flux.onecold(probs)\n",
    "pred_classes = labelnames[o]\n",
    "print(mean(pred_classes .== test_y))"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "we see the accuracy is around 79% on test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "#predicted classes for test set\n",
    "print(pred_classes)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "gt classes for test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "print(test_y)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "probabilities for test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.9805322 0.2384057 0.9637127 0.19163147 0.28530887 0.9786587 0.23583542 0.2623012 0.9710335 0.96257037 0.975023 0.1800483 0.26517195 0.9785461 0.97219276 0.26467112 0.975127 0.18752997 0.16941528 0.26280475 0.090576045 0.8618955 0.97741437 0.86254597 0.37131914 0.97425836 0.75622874 0.17619236 0.93809325 0.9727401 0.9505506 0.21099679 0.08683983 0.25472873 0.96059674 0.9785876 0.96645653 0.95931476 0.8683027 0.23834394 0.9742777 0.9728862 0.31394815 0.98282164; 0.019467814 0.7615943 0.03628733 0.80836856 0.71469116 0.021341389 0.7641646 0.73769885 0.028966505 0.037429623 0.02497704 0.8199517 0.7348281 0.021453913 0.027807215 0.7353289 0.024873 0.8124701 0.8305847 0.73719525 0.9094239 0.13810456 0.022585655 0.13745403 0.6286808 0.02574167 0.24377124 0.8238077 0.061906718 0.02725992 0.04944938 0.7890032 0.91316015 0.74527127 0.039403267 0.02141239 0.03354348 0.04068529 0.13169727 0.76165605 0.025722267 0.027113833 0.68605185 0.017178303]"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "print(probs)"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
