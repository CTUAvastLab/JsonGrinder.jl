<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mutagenesis Example · JsonGrinder.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="JsonGrinder.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="JsonGrinder.jl logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../schema/">Schema</a></li><li><a class="tocitem" href="../../extractors/">Creating extractors</a></li><li><a class="tocitem" href="../../exfunctions/">Extractors overview</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../">Examples Overview</a></li><li class="is-active"><a class="tocitem" href>Mutagenesis Example</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Create-the-model"><span>Create the model</span></a></li><li class="toplevel"><a class="tocitem" href="#Train-the-model"><span>Train the model</span></a></li><li class="toplevel"><a class="tocitem" href="#Classify-test-set"><span>Classify test set</span></a></li></ul></li><li><a class="tocitem" href="../recipes/">Recipe Ingredients Example</a></li><li><a class="tocitem" href="../schema_examination/">Schema Examination</a></li><li><a class="tocitem" href="../schema_visualization/">Schema Visualization</a></li></ul></li><li><a class="tocitem" href="../../automl/">AutoML</a></li><li><a class="tocitem" href="../../hierarchical/">External tools</a></li><li><a class="tocitem" href="../../api/">API Documentation</a></li><li><a class="tocitem" href="../../developers/">Developers</a></li><li><a class="tocitem" href="../../citation/">Citation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Mutagenesis Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mutagenesis Example</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/CTUAvastLab/JsonGrinder.jl/blob/master/examples/mutagenesis.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Mutagenesis-Example"><a class="docs-heading-anchor" href="#Mutagenesis-Example">Mutagenesis Example</a><a id="Mutagenesis-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Mutagenesis-Example" title="Permalink"></a></h1><p>Following example demonstrates learning to <a href="https://relational.fit.cvut.cz/dataset/Mutagenesis">predict the mutagenicity on Salmonella typhimurium</a> (dataset is stored in json format <a href="https://juliaml.github.io/MLDatasets.jl/stable/datasets/Mutagenesis/">in MLDatasets.jl</a> for your convenience).</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>This example is also available as a Jupyter notebook, feel free to run it yourself: <a href="https://nbviewer.jupyter.org/github/CTUAvastLab/JsonGrinder.jl/blob/gh-pages/dev/examples/mutagenesis.ipynb"><code>mutagenesis.ipynb</code></a></p></div></div><p>Here we include libraries all necessary libraries</p><pre><code class="language-julia hljs">using JsonGrinder, MLDatasets, Flux, Mill, MLDataPattern, Statistics, ChainRulesCore</code></pre><pre><code class="nohighlight hljs">[ Info: Installing scipy via the Conda scipy package...
[ Info: Running `conda install -q -y scipy` in root environment
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... done

## Package Plan ##

  environment location: /home/runner/.julia/conda/3

  added / updated specs:
    - scipy


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB
    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB
    scipy-1.7.3                |   py39hc147768_0        16.9 MB
    ------------------------------------------------------------
                                           Total:        17.9 MB

The following NEW packages will be INSTALLED:

  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.5.0-ha8ba4b0_17
  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17
  scipy              pkgs/main/linux-64::scipy-1.7.3-py39hc147768_0


Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
</code></pre><p>Here we load all samples.</p><pre><code class="language-julia hljs">train_x, train_y = MLDatasets.Mutagenesis.traindata();
test_x, test_y = MLDatasets.Mutagenesis.testdata();
nothing #hide</code></pre><p>We define some basic parameters for the construction and training of the neural network. Minibatch size is self-explanatory, iterations is number of iterations of gradient descent Neurons is number of neurons in hidden layers for each version of part of the neural network.</p><pre><code class="language-julia hljs">minibatchsize = 100
iterations = 5_000
neurons = 20</code></pre><pre><code class="nohighlight hljs">20</code></pre><p>We create the schema of the training data, which is the first important step in using the JsonGrinder. This computes both the structure (also known as JSON schema) and histogram of occurrences of individual values in the training data.</p><pre><code class="language-julia hljs">sch = JsonGrinder.schema(train_x)</code></pre><pre><code class="nohighlight hljs">[Dict] 	# updated = 100
  ├─── lumo: [Scalar - Float64], 98 unique values 	# updated = 100
  ├─── inda: [Scalar - Int64], 1 unique values 	# updated = 100
  ⋮
  └── atoms: [List] 	# updated = 100
               └── [Dict] 	# updated = 2529
                     ⋮</code></pre><p>Then we use it to create the extractor converting jsons to Mill structures. The <code>suggestextractor</code> is executed below with default setting, but it allows you heavy customization.</p><pre><code class="language-julia hljs">extractor = suggestextractor(sch)</code></pre><pre><code class="nohighlight hljs">Dict
  ├─── lumo: Categorical d = 99
  ├─── inda: Categorical d = 2
  ⋮
  └── atoms: Array of
               └── Dict
                     ⋮</code></pre><p>We convert jsons to mill data samples and prepare list of classes. This classification problem is two-class, but we want to infer it from labels. The extractor is callable, so we can pass it vector of samples to obtain vector of structures with extracted features.</p><pre><code class="language-julia hljs">train_data = extractor.(train_x)
test_data = extractor.(test_x)
labelnames = unique(train_y)</code></pre><pre><code class="nohighlight hljs">2-element Vector{Int64}:
 1
 0</code></pre><h1 id="Create-the-model"><a class="docs-heading-anchor" href="#Create-the-model">Create the model</a><a id="Create-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Create-the-model" title="Permalink"></a></h1><p>We create the model reflecting structure of the data</p><pre><code class="language-julia hljs">model = reflectinmodel(sch, extractor,
	layer -&gt; Dense(layer, neurons, relu),
	bag -&gt; SegmentedMeanMax(bag),
	fsm = Dict(&quot;&quot; =&gt; layer -&gt; Dense(layer, length(labelnames))),
)</code></pre><pre><code class="nohighlight hljs">ProductModel ↦ ArrayModel(Dense(100, 2)) 	# 2 arrays, 202 params, 888 bytes
  ├─── lumo: ArrayModel(Dense(99, 20, relu)) 	# 2 arrays, 2_000 params, 7.891 KiB
  ├─── inda: ArrayModel(Dense(2, 20, relu)) 	# 2 arrays, 60 params, 320 bytes
  ├─── logp: ArrayModel(Dense(63, 20, relu)) 	# 2 arrays, 1_280 params, 5.078 KiB
  ├─── ind1: ArrayModel(Dense(3, 20, relu)) 	# 2 arrays, 80 params, 400 bytes
  └── atoms: BagModel ↦ [SegmentedMean(20); SegmentedMax(20)] ↦ ArrayModel(Dense(40, 20, relu)) 	# 4 arrays, 860 params, 3.516 KiB
               └── ProductModel ↦ ArrayModel(Dense(61, 20, relu)) 	# 2 arrays, 1_240 params, 4.922 KiB
                     ⋮</code></pre><p>this allows us to create model flexibly, without the need to hardcode individual layers. Individual arguments of <code>reflectinmodel</code> are explained in <a href="https://CTUAvastLab.github.io/Mill.jl/dev/manual/reflectin/#Model-Reflection">Mill.jl documentation</a>. But briefly: for every numeric array in the sample, model will create a dense layer with <code>neurons</code> neurons (20 in this example). For every vector of observations (called bag in Multiple Instance Learning terminology), it will create aggregation function which will take mean, maximum of feature vectors and concatenate them. The <code>fsm</code> keyword argument basically says that on the end of the NN, as a last layer, we want 2 neurons <code>length(labelnames)</code> in the output layer, not 20 as in the intermediate layers.</p><h1 id="Train-the-model"><a class="docs-heading-anchor" href="#Train-the-model">Train the model</a><a id="Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-model" title="Permalink"></a></h1><p>Then, we define few handy functions and a loss function, which is categorical crossentropy in our case.</p><pre><code class="language-julia hljs">loss(x,y) = Flux.logitcrossentropy(inference(x), Flux.onehotbatch(y, labelnames))
inference(x::AbstractMillNode) = model(x).data
inference(x::AbstractVector{&lt;:AbstractMillNode}) = inference(reduce(catobs, x))
accuracy(x,y) = mean(labelnames[Flux.onecold(inference(x))] .== y)
loss(xy::Tuple) = loss(xy...)
@non_differentiable Base.reduce(catobs, x::AbstractVector{&lt;:AbstractMillNode})</code></pre><p>And we can add a callback which will be printing train and test accuracy during the training and then we can start trining</p><pre><code class="language-julia hljs">cb = () -&gt; begin
	train_acc = accuracy(train_data, train_y)
	test_acc = accuracy(test_data, test_y)
	println(&quot;accuracy: train = $train_acc, test = $test_acc&quot;)
end</code></pre><pre><code class="nohighlight hljs">#9 (generic function with 1 method)</code></pre><p>Lastly we turn our training data to minibatches, and we can start training</p><pre><code class="language-julia hljs">minibatches = RandomBatches((train_data, train_y), size = minibatchsize, count = iterations)
Flux.Optimise.train!(loss, Flux.params(model), minibatches, ADAM(), cb = Flux.throttle(cb, 2))</code></pre><pre><code class="nohighlight hljs">accuracy: train = 0.7, test = 0.3409090909090909
accuracy: train = 0.79, test = 0.8636363636363636
accuracy: train = 0.83, test = 0.8863636363636364
accuracy: train = 0.82, test = 0.8863636363636364
accuracy: train = 0.82, test = 0.8863636363636364
accuracy: train = 0.82, test = 0.8863636363636364
accuracy: train = 0.82, test = 0.8863636363636364
accuracy: train = 0.83, test = 0.8863636363636364
accuracy: train = 0.84, test = 0.8863636363636364
accuracy: train = 0.83, test = 0.8863636363636364
accuracy: train = 0.83, test = 0.8863636363636364
accuracy: train = 0.84, test = 0.8863636363636364
accuracy: train = 0.84, test = 0.8863636363636364
accuracy: train = 0.84, test = 0.8863636363636364
accuracy: train = 0.85, test = 0.8863636363636364
accuracy: train = 0.84, test = 0.8863636363636364
accuracy: train = 0.85, test = 0.8863636363636364
accuracy: train = 0.86, test = 0.8863636363636364
accuracy: train = 0.86, test = 0.8863636363636364
accuracy: train = 0.87, test = 0.8863636363636364
accuracy: train = 0.87, test = 0.8863636363636364
accuracy: train = 0.87, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
accuracy: train = 0.89, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
accuracy: train = 0.9, test = 0.8863636363636364
</code></pre><p>We can see the accuracy rising and obtaining over 98% on training set quite quickly, and on test set we get over 70%.</p><h1 id="Classify-test-set"><a class="docs-heading-anchor" href="#Classify-test-set">Classify test set</a><a id="Classify-test-set-1"></a><a class="docs-heading-anchor-permalink" href="#Classify-test-set" title="Permalink"></a></h1><p>The Last part is inference on test data.</p><pre><code class="language-julia hljs">probs = softmax(inference(test_data))
o = Flux.onecold(probs)
pred_classes = labelnames[o]
mean(pred_classes .== test_y)</code></pre><pre><code class="nohighlight hljs">0.8863636363636364</code></pre><p><code>pred_classes</code> contains the predictions for our test set. we see the accuracy is around 75% on test set predicted classes for test set</p><pre><code class="language-julia hljs">pred_classes</code></pre><pre><code class="nohighlight hljs">44-element Vector{Int64}:
 1
 0
 1
 0
 0
 1
 0
 0
 1
 1
 1
 0
 0
 1
 1
 0
 1
 0
 0
 0
 0
 1
 1
 1
 0
 1
 1
 0
 1
 1
 1
 0
 0
 0
 1
 1
 1
 1
 1
 0
 1
 1
 0
 1</code></pre><p>Ground truth classes for test set</p><pre><code class="language-julia hljs">test_y</code></pre><pre><code class="nohighlight hljs">44-element Vector{Int64}:
 1
 1
 1
 0
 1
 1
 0
 0
 1
 1
 1
 0
 0
 1
 1
 0
 1
 1
 0
 0
 0
 1
 1
 1
 1
 1
 1
 0
 1
 1
 1
 0
 0
 0
 1
 1
 1
 1
 1
 1
 1
 1
 0
 1</code></pre><p>probabilities for test set</p><pre><code class="language-julia hljs">probs</code></pre><pre><code class="nohighlight hljs">2×44 Matrix{Float32}:
 0.931987   0.181162  0.913194   0.137565  0.197385  0.938881   0.306633  0.334883  0.750332  0.926249   0.914144   0.167432  0.191422  0.931937   0.913401   0.332354  0.919058   0.167978  0.137781  0.334577  0.370845  0.890715  0.929065  0.940629   0.412945  0.955763   0.729317  0.215848  0.920569  0.928279   0.914794   0.175786  0.337784  0.169299  0.925102   0.926008  0.940184   0.927226   0.944992   0.18122  0.957328  0.915105   0.0468927  0.897522
 0.0680129  0.818838  0.0868059  0.862435  0.802615  0.0611192  0.693367  0.665117  0.249668  0.0737514  0.0858561  0.832568  0.808578  0.0680629  0.0865985  0.667646  0.0809416  0.832022  0.862219  0.665424  0.629155  0.109285  0.070935  0.0593711  0.587055  0.0442373  0.270683  0.784152  0.079431  0.0717208  0.0852061  0.824214  0.662216  0.830701  0.0748978  0.073992  0.0598163  0.0727739  0.0550076  0.81878  0.042672  0.0848947  0.953107   0.102479</code></pre><p>We can look at individual samples. For instance, some sample from test set is</p><pre><code class="language-julia hljs">test_data[2]</code></pre><pre><code class="nohighlight hljs">ProductNode 	# 1 obs, 104 bytes
  ├─── lumo: ArrayNode(99×1 OneHotArray with Bool elements) 	# 1 obs, 60 bytes
  ├─── inda: ArrayNode(2×1 OneHotArray with Bool elements) 	# 1 obs, 60 bytes
  ├─── logp: ArrayNode(63×1 OneHotArray with Bool elements) 	# 1 obs, 60 bytes
  ├─── ind1: ArrayNode(3×1 OneHotArray with Bool elements) 	# 1 obs, 60 bytes
  └── atoms: BagNode 	# 1 obs, 136 bytes
               └── ProductNode 	# 24 obs, 64 bytes
                     ⋮</code></pre><p>and the corresponding classification is</p><pre><code class="language-julia hljs">pred_classes[2]</code></pre><pre><code class="nohighlight hljs">0</code></pre><p>if you want to see the probability distribution, it can be obtained by applying <code>softmax</code> to the output of the network.</p><pre><code class="language-julia hljs">softmax(model(test_data[2]).data)</code></pre><pre><code class="nohighlight hljs">2×1 Matrix{Float32}:
 0.18116154
 0.8188385</code></pre><p>so we can see that the probability that given sample is <code>mutagenetic</code> is almost 1.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Examples Overview</a><a class="docs-footer-nextpage" href="../recipes/">Recipe Ingredients Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Tuesday 11 January 2022 09:35">Tuesday 11 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
